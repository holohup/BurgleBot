{
  "schema": 1,
  "description": "Checks the result of LLM inference",
  "execution_settings": {
    "default": {
      "max_tokens": 4096,
      "temperature": 0.0,
      "top_p": 0.1,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0
    }
  },
  "input_variables": [
    {
      "name": "prompt",
      "description": "Prompt for LLM",
      "default": "",
      "is_required": true
    },
    {
      "name": "input",
      "description": "Text input for the plugin",
      "default": "",
      "is_required": true
    }
  ]
}